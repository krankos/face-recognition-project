{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./dataset\\\\khalil\\\\image00007.jpeg' './dataset\\\\khalil\\\\image00008.jpeg'\n",
      " './dataset\\\\khalil\\\\image00020.jpeg' './dataset\\\\khalil\\\\image00021.jpeg'\n",
      " './dataset\\\\khalil\\\\image00022.jpeg' './dataset\\\\khalil\\\\image00033.jpeg'\n",
      " './dataset\\\\khalil\\\\image00038.jpeg'\n",
      " './dataset\\\\khalil\\\\photo_2022-11-06_14-49-18.jpg'\n",
      " './dataset\\\\khalil\\\\photo_2022-11-06_14-50-06.jpg'\n",
      " './dataset\\\\others\\\\106.jpg' './dataset\\\\others\\\\29.jpg'\n",
      " './dataset\\\\others\\\\332.jpg' './dataset\\\\others\\\\409.jpg'\n",
      " './dataset\\\\others\\\\43.jpg' './dataset\\\\others\\\\80.jpg'\n",
      " './dataset\\\\others\\\\90.jpg']\n",
      "['khalil' 'khalil' 'khalil' 'khalil' 'khalil' 'khalil' 'khalil' 'khalil'\n",
      " 'khalil' 'others' 'others' 'others' 'others' 'others' 'others' 'others']\n"
     ]
    }
   ],
   "source": [
    "# images are in ./dataset/khalil and ./dataset/others\n",
    "# labels are the folder names\n",
    "# load the data\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "data_path = './dataset'\n",
    "folders = os.listdir(data_path)\n",
    "labels = []\n",
    "images = []\n",
    "\n",
    "# each image has a label. labels are the folder names.\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(data_path, folder)\n",
    "    for image in os.listdir(folder_path):\n",
    "        image_path = os.path.join(folder_path, image)\n",
    "        images.append(image_path)\n",
    "        labels.append(folder)\n",
    "\n",
    "# convert to numpy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "print(images)\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HaaR Cascade Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:9: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:9: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "C:\\Users\\MSI\\AppData\\Local\\Temp\\ipykernel_20984\\3664264089.py:9: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if faces is ():\n"
     ]
    }
   ],
   "source": [
    "# detect faces in the image and extract the face\n",
    "\n",
    "import cv2\n",
    "haar_file = 'haarcascade_frontalface_default.xml'\n",
    "face_cascade = cv2.CascadeClassifier(haar_file)\n",
    "\n",
    "def face_extractor(img):\n",
    "    faces = face_cascade.detectMultiScale(img, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return None\n",
    "    for (x, y, w, h) in faces:\n",
    "        cropped_face = img[y:y+h, x:x+w]\n",
    "    return cropped_face\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:4: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "C:\\Users\\MSI\\AppData\\Local\\Temp\\ipykernel_20984\\4126523297.py:4: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if faces is ():\n"
     ]
    }
   ],
   "source": [
    "# function to extract faces from an image\n",
    "def faces_extractor(img):\n",
    "    faces = face_cascade.detectMultiScale(img, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return None\n",
    "    faces_list = []\n",
    "    for (x, y, w, h) in faces:\n",
    "        cropped_face = img[y:y+h, x:x+w]\n",
    "        faces_list.append(cropped_face)\n",
    "    return faces_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test faces_extractor on \"./dataset/many.jpeg\"\n",
    "# img = cv2.imread('./dataset/many.jpeg')\n",
    "# faces = faces_extractor(img)\n",
    "# for face in faces:\n",
    "#     cv2.imshow('face', face)\n",
    "#     cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# extract faces from the images and display them\n",
    "for image in images:\n",
    "    img = cv2.imread(image)\n",
    "    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    face = face_extractor(img)\n",
    "    if face is not None:\n",
    "        face = cv2.resize(face, (200, 200))\n",
    "        imgplot = plt.imshow(face)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract faces from the images using faces_extractor and display them\n",
    "# for image in images:\n",
    "#     img = cv2.imread(image)\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#     faces = faces_extractor(img)\n",
    "#     for face in faces:\n",
    "#         if face is not None:\n",
    "#             face = cv2.resize(face, (200, 200))\n",
    "#             imgplot = plt.imshow(face)\n",
    "#             plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "for image in images:\n",
    "    img = mpimg.imread(image)\n",
    "\n",
    "    face = face_extractor(img)\n",
    "    if face is not None:\n",
    "        plt.imshow(face)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('Face not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the faces in \"processed\" folder. The folder will be created if it doesn't exist.\n",
    "# the faces are saved with the name of the folder they belong to (label) and a number.\n",
    "processed_path = './processed'\n",
    "if not os.path.exists(processed_path):\n",
    "    os.mkdir(processed_path)\n",
    "#clean the folder\n",
    "for folder in os.listdir(processed_path):\n",
    "    folder_path = os.path.join(processed_path, folder)\n",
    "    for file in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        os.remove(file_path)\n",
    "    os.rmdir(folder_path)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "for i, image in enumerate(images):\n",
    "    img = cv2.imread(image)\n",
    "    face = face_extractor(img)\n",
    "    if face is not None:\n",
    "        face = cv2.resize(face, (200, 200))\n",
    "    \n",
    "        save_path = os.path.join(processed_path, labels[i])\n",
    "        if not os.path.exists(save_path):\n",
    "            os.mkdir(save_path)\n",
    "        file_name_path = os.path.join(save_path, labels[i] + str(i) + '.jpg')\n",
    "        cv2.imwrite(file_name_path, face)\n",
    "        cv2.waitKey(0)\n",
    "    else:\n",
    "        print('Face not found')\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./processed/khalil/'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed= './processed/khalil/'\n",
    "processed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #using faces_extractor\n",
    "# processed_path = './processed'\n",
    "# if not os.path.exists(processed_path):\n",
    "#     os.mkdir(processed_path)\n",
    "# #clean the folder\n",
    "# for folder in os.listdir(processed_path):\n",
    "#     folder_path = os.path.join(processed_path, folder)\n",
    "#     for file in os.listdir(folder_path):\n",
    "#         file_path = os.path.join(folder_path, file)\n",
    "#         os.remove(file_path)\n",
    "#     os.rmdir(folder_path)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.image as mpimg\n",
    "\n",
    "# for i, image in enumerate(images):\n",
    "#     img = cv2.imread(image)\n",
    "#     faces = faces_extractor(img)\n",
    "#     for j, face in enumerate(faces):\n",
    "#         if face is not None:\n",
    "#             face = cv2.resize(face, (200, 200))\n",
    "#             face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "#             save_path = os.path.join(processed_path, labels[i])\n",
    "#             if not os.path.exists(save_path):\n",
    "#                 os.mkdir(save_path)\n",
    "#             file_name_path = os.path.join(save_path, labels[i] + str(i) + '_' + str(j) + '.jpg')\n",
    "#             cv2.imwrite(file_name_path, face)\n",
    "#             cv2.waitKey(0)\n",
    "#         else:\n",
    "#             print('Face not found')\n",
    "#             pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 images belonging to 2 classes.\n",
      "Found 16 images belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'khalil': 0, 'others': 1}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deep Learning CNN model to recognize face\n",
    "'''This script uses a database of images and creates CNN model on top of it to test\n",
    "   if the given image is recognized correctly or not'''\n",
    "\n",
    "'''####### IMAGE PRE-PROCESSING for TRAINING and TESTING data #######'''\n",
    "\n",
    "# Specifying the folder where images are present\n",
    "TrainingImagePath='./processed'\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# Understand more about ImageDataGenerator at below link\n",
    "# https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "\n",
    "# Defining pre-processing transformations on raw images of training data\n",
    "# These hyper parameters helps to generate slightly twisted versions\n",
    "# of the original image, which leads to a better model, since it learns\n",
    "# on the good and bad mix of images\n",
    "train_datagen = ImageDataGenerator(\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# Defining pre-processing transformations on raw images of testing data\n",
    "# No transformations are done on the testing images\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "# Generating the Training Data\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        TrainingImagePath,\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "\n",
    "# Generating the Testing Data\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        TrainingImagePath,\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "# Printing class labels for each face\n",
    "test_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DirectoryIterator' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\study\\IRM 2\\face-reco\\notebook.ipynb Cell 28\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m training_set\u001b[39m.\u001b[39;49mvalues\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DirectoryIterator' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "training_set.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ImageDataGenerator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\study\\IRM 2\\face-reco\\notebook.ipynb Cell 29\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#Y120sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# clear the output folder before running the model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#Y120sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# folder_path = './output'\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#Y120sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# for file in os.listdir(folder_path):\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#Y120sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#     file_path = os.path.join(folder_path, file)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#Y120sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#     os.remove(file_path)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#Y120sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# os.rmdir(folder_path)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#Y120sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m datagen \u001b[39m=\u001b[39m ImageDataGenerator(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#Y120sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     rescale\u001b[39m=\u001b[39m\u001b[39m1.\u001b[39m\u001b[39m/\u001b[39m\u001b[39m255\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#Y120sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     rotation_range\u001b[39m=\u001b[39m\u001b[39m40\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#Y120sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     width_shift_range\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#Y120sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     height_shift_range\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#Y120sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     shear_range\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#Y120sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     zoom_range\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#Y120sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     horizontal_flip\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#Y120sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#Y120sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m dir_Iterator \u001b[39m=\u001b[39m datagen\u001b[39m.\u001b[39mflow_from_directory(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#Y120sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     TrainingImagePath,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#Y120sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#Y120sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     target_size\u001b[39m=\u001b[39m(\u001b[39m256\u001b[39m, \u001b[39m256\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#Y120sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#Y120sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ImageDataGenerator' is not defined"
     ]
    }
   ],
   "source": [
    "# clear the output folder before running the model\n",
    "# folder_path = './output'\n",
    "# for file in os.listdir(folder_path):\n",
    "#     file_path = os.path.join(folder_path, file)\n",
    "#     os.remove(file_path)\n",
    "# os.rmdir(folder_path)\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "dir_Iterator = datagen.flow_from_directory(\n",
    "    TrainingImagePath,\n",
    "    batch_size=32,\n",
    "    save_to_dir=\"output/\",\n",
    "    save_format=\"jpg\",\n",
    "    target_size=(256, 256),\n",
    ")\n",
    "for i in range(10):\n",
    "    img, label = dir_Iterator.next()\n",
    "    print(img.shape, label)\n",
    "    plt.imshow(img[0])\n",
    "    plt.show()\n",
    "\n",
    "training_set=dir_Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_dataset_from_directory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\MALEK\\Documents\\GitHub\\face-recognition-project\\notebook.ipynb Cellule 30\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MALEK/Documents/GitHub/face-recognition-project/notebook.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# generate training data\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MALEK/Documents/GitHub/face-recognition-project/notebook.ipynb#X51sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m preprocessing \n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/MALEK/Documents/GitHub/face-recognition-project/notebook.ipynb#X51sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m training_images \u001b[39m=\u001b[39m image_dataset_from_directory(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MALEK/Documents/GitHub/face-recognition-project/notebook.ipynb#X51sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     TrainingImagePath,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MALEK/Documents/GitHub/face-recognition-project/notebook.ipynb#X51sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     labels\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39minferred\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MALEK/Documents/GitHub/face-recognition-project/notebook.ipynb#X51sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     class_names\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mkhalil\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mother\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MALEK/Documents/GitHub/face-recognition-project/notebook.ipynb#X51sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     color_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrgb\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MALEK/Documents/GitHub/face-recognition-project/notebook.ipynb#X51sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MALEK/Documents/GitHub/face-recognition-project/notebook.ipynb#X51sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MALEK/Documents/GitHub/face-recognition-project/notebook.ipynb#X51sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m#display the images\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MALEK/Documents/GitHub/face-recognition-project/notebook.ipynb#X51sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m training_images\u001b[39m.\u001b[39mtake(\u001b[39m1\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_dataset_from_directory' is not defined"
     ]
    }
   ],
   "source": [
    "# generate training data\n",
    "from keras import preprocessing \n",
    "training_images = image_dataset_from_directory(\n",
    "    TrainingImagePath,\n",
    "    labels='inferred',\n",
    "    class_names=['khalil', 'other'],\n",
    "    color_mode='rgb',\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "#display the images\n",
    "for x, y in training_images.take(1):\n",
    "    plt.imshow(x[0])\n",
    "    plt.show()  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32me:\\study\\IRM 2\\face-reco\\notebook.ipynb Cell 31\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#Y133sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtf\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage\u001b[39;00m \u001b[39mimport\u001b[39;00m ImageDataGenerator\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#Y133sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m TrainingImagePath\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./processed\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#Y133sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m datagen \u001b[39m=\u001b[39m ImageDataGenerator(\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#Y133sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     rescale\u001b[39m=\u001b[39m\u001b[39m1.\u001b[39m\u001b[39m/\u001b[39m\u001b[39m255\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#Y133sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     rotation_range\u001b[39m=\u001b[39m\u001b[39m40\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#Y133sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     horizontal_flip\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/study/IRM%202/face-reco/notebook.ipynb#Y133sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tf'"
     ]
    }
   ],
   "source": [
    "from tf.keras.preprocessing.image import ImageDataGenerator\n",
    "TrainingImagePath='./processed'\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "dir_Iterator = datagen.flow_from_directory(\n",
    "    TrainingImagePath,\n",
    "    batch_size=32,\n",
    "    save_to_dir=\"output/\",\n",
    "    save_format=\"jpg\",\n",
    "    target_size=(256, 256),\n",
    ")\n",
    "for i in range(10):\n",
    "    img, label = dir_Iterator.next()\n",
    "    print(img.shape, label)\n",
    "    plt.imshow(img[0])\n",
    "    plt.show()\n",
    "\n",
    "training_set=dir_Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a mapping for index and face names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of Face and its ID {0: 'khalil', 1: 'others'}\n",
      "\n",
      " The Number of output neurons:  2\n"
     ]
    }
   ],
   "source": [
    "# class_indices have the numeric tag for each face\n",
    "TrainClasses=training_set.class_indices\n",
    " \n",
    "# Storing the face and the numeric tag for future reference\n",
    "ResultMap={}\n",
    "for faceValue,faceName in zip(TrainClasses.values(),TrainClasses.keys()):\n",
    "    ResultMap[faceValue]=faceName\n",
    "\n",
    "# Print the ResultMap\n",
    "print(ResultMap)\n",
    "\n",
    "# Saving the face map for future reference\n",
    "import pickle\n",
    "with open(\"ResultsMap.pkl\", 'wb') as fileWriteStream:\n",
    "    pickle.dump(ResultMap, fileWriteStream)\n",
    " \n",
    "# The model will give answer as a numeric tag\n",
    "# This mapping will help to get the corresponding face name for it\n",
    "print(\"Mapping of Face and its ID\",ResultMap)\n",
    " \n",
    "# The number of neurons for the output layer is equal to the number of faces\n",
    "OutputNeurons=len(ResultMap)\n",
    "print('\\n The Number of output neurons: ', OutputNeurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the CNN face recognition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MALEK\\AppData\\Local\\Temp\\ipykernel_13384\\2540500369.py:43: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  classifier.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_train_function.<locals>.train_function at 0x00000148390A1900> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      " 1/30 [>.............................] - ETA: 1:05 - loss: 0.6749 - accuracy: 0.6250WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 300 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential_4/flatten_4/Reshape' defined at (most recent call last):\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\traitlets\\config\\application.py\", line 976, in launch_instance\n      app.start()\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\MALEK\\AppData\\Local\\Temp\\ipykernel_13384\\2540500369.py\", line 43, in <cell line: 43>\n      classifier.fit_generator(\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2507, in fit_generator\n      return self.fit(\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1606, in fit\n      val_logs = self.evaluate(\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1947, in evaluate\n      tmp_logs = self.test_function(iterator)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1727, in test_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1713, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1701, in run_step\n      outputs = model.test_step(data)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1665, in test_step\n      y_pred = self(x, training=False)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\sequential.py\", line 410, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\layers\\reshaping\\flatten.py\", line 104, in call\n      return tf.reshape(inputs, flattened_shape)\nNode: 'sequential_4/flatten_4/Reshape'\nInput to reshape is a tensor with 173056 values, but the requested shape requires a multiple of 238144\n\t [[{{node sequential_4/flatten_4/Reshape}}]] [Op:__inference_test_function_5914]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\MALEK\\Documents\\GitHub\\face-recognition-project\\notebook.ipynb Cellule 34\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MALEK/Documents/GitHub/face-recognition-project/notebook.ipynb#X42sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m StartTime\u001b[39m=\u001b[39mtime\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MALEK/Documents/GitHub/face-recognition-project/notebook.ipynb#X42sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# Starting the model training\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/MALEK/Documents/GitHub/face-recognition-project/notebook.ipynb#X42sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m classifier\u001b[39m.\u001b[39;49mfit_generator(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MALEK/Documents/GitHub/face-recognition-project/notebook.ipynb#X42sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m                     training_set,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MALEK/Documents/GitHub/face-recognition-project/notebook.ipynb#X42sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m                     steps_per_epoch\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MALEK/Documents/GitHub/face-recognition-project/notebook.ipynb#X42sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m                     epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MALEK/Documents/GitHub/face-recognition-project/notebook.ipynb#X42sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m                     validation_data\u001b[39m=\u001b[39;49mtest_set,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MALEK/Documents/GitHub/face-recognition-project/notebook.ipynb#X42sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m                     validation_steps\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MALEK/Documents/GitHub/face-recognition-project/notebook.ipynb#X42sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m EndTime\u001b[39m=\u001b[39mtime\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MALEK/Documents/GitHub/face-recognition-project/notebook.ipynb#X42sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m###### Total Time Taken: \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mround\u001b[39m((EndTime\u001b[39m-\u001b[39mStartTime)\u001b[39m/\u001b[39m\u001b[39m60\u001b[39m), \u001b[39m'\u001b[39m\u001b[39mMinutes ######\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:2507\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2495\u001b[0m \u001b[39m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[0;32m   2496\u001b[0m \n\u001b[0;32m   2497\u001b[0m \u001b[39mDEPRECATED:\u001b[39;00m\n\u001b[0;32m   2498\u001b[0m \u001b[39m  `Model.fit` now supports generators, so there is no longer any need to\u001b[39;00m\n\u001b[0;32m   2499\u001b[0m \u001b[39m  use this endpoint.\u001b[39;00m\n\u001b[0;32m   2500\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2501\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   2502\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m`Model.fit_generator` is deprecated and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2503\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mwill be removed in a future version. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2504\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2505\u001b[0m     stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m   2506\u001b[0m )\n\u001b[1;32m-> 2507\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m   2508\u001b[0m     generator,\n\u001b[0;32m   2509\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   2510\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m   2511\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   2512\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   2513\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[0;32m   2514\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[0;32m   2515\u001b[0m     validation_freq\u001b[39m=\u001b[39;49mvalidation_freq,\n\u001b[0;32m   2516\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[0;32m   2517\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   2518\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   2519\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   2520\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   2521\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[0;32m   2522\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential_4/flatten_4/Reshape' defined at (most recent call last):\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\traitlets\\config\\application.py\", line 976, in launch_instance\n      app.start()\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\MALEK\\AppData\\Local\\Temp\\ipykernel_13384\\2540500369.py\", line 43, in <cell line: 43>\n      classifier.fit_generator(\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2507, in fit_generator\n      return self.fit(\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1606, in fit\n      val_logs = self.evaluate(\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1947, in evaluate\n      tmp_logs = self.test_function(iterator)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1727, in test_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1713, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1701, in run_step\n      outputs = model.test_step(data)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1665, in test_step\n      y_pred = self(x, training=False)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\sequential.py\", line 410, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\MALEK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\layers\\reshaping\\flatten.py\", line 104, in call\n      return tf.reshape(inputs, flattened_shape)\nNode: 'sequential_4/flatten_4/Reshape'\nInput to reshape is a tensor with 173056 values, but the requested shape requires a multiple of 238144\n\t [[{{node sequential_4/flatten_4/Reshape}}]] [Op:__inference_test_function_5914]"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    " \n",
    "'''Initializing the Convolutional Neural Network'''\n",
    "classifier= Sequential()\n",
    " \n",
    "''' STEP--1 Convolution\n",
    "# Adding the first layer of CNN\n",
    "# we are using the format (64,64,3) because we are using TensorFlow backend\n",
    "# It means 3 matrix of size (64X64) pixels representing Red, Green and Blue components of pixels\n",
    "'''\n",
    "classifier.add(Convolution2D(32, kernel_size=(5, 5), strides=(1, 1), input_shape=(256,256,3), activation='relu'))\n",
    " \n",
    "'''# STEP--2 MAX Pooling'''\n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
    " \n",
    "'''############## ADDITIONAL LAYER of CONVOLUTION for better accuracy #################'''\n",
    "classifier.add(Convolution2D(64, kernel_size=(5, 5), strides=(1, 1), activation='relu'))\n",
    " \n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
    " \n",
    "'''# STEP--3 FLattening'''\n",
    "classifier.add(Flatten())\n",
    " \n",
    "'''# STEP--4 Fully Connected Neural Network'''\n",
    "classifier.add(Dense(64, activation='relu'))\n",
    " \n",
    "classifier.add(Dense(OutputNeurons, activation='softmax'))\n",
    " \n",
    "'''# Compiling the CNN'''\n",
    "#classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=[\"accuracy\"])\n",
    " \n",
    "###########################################################\n",
    "import time\n",
    "# Measuring the time taken by the model to train\n",
    "StartTime=time.time()\n",
    " \n",
    "# Starting the model training\n",
    "classifier.fit_generator(\n",
    "                    training_set,\n",
    "                    steps_per_epoch=30,\n",
    "                    epochs=10,\n",
    "                    validation_data=test_set,\n",
    "                    validation_steps=10)\n",
    " \n",
    "EndTime=time.time()\n",
    "print(\"###### Total Time Taken: \", round((EndTime-StartTime)/60), 'Minutes ######')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImagePath='/khalil0.jpg'\n",
    "im=cv2.imread(ImagePath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Prediction is:  khalil\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras.utils as image\n",
    " \n",
    "ImagePath='others9.jpg'\n",
    "test_image=image.load_img(ImagePath,target_size=(64, 64))\n",
    "test_image=image.img_to_array(test_image)\n",
    " \n",
    "test_image=np.expand_dims(test_image,axis=0)\n",
    " \n",
    "result=classifier.predict(test_image,verbose=0)\n",
    "#print(training_set.class_indices)\n",
    " \n",
    "print('####'*10)\n",
    "print('Prediction is: ',ResultMap[np.argmax(result)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff39798ba2f49d9c8c85df32f9bbb83f5fb04a27d2e6570577e90ab0f793dad2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
