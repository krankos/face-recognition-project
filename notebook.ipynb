{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images are in ./dataset/khalil and ./dataset/others\n",
    "# labels are the folder names\n",
    "# load the data\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "data_path = './dataset'\n",
    "folders = os.listdir(data_path)\n",
    "labels = []\n",
    "images = []\n",
    "\n",
    "# each image has a label. labels are the folder names.\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(data_path, folder)\n",
    "    for image in os.listdir(folder_path):\n",
    "        image_path = os.path.join(folder_path, image)\n",
    "        images.append(image_path)\n",
    "        labels.append(folder)\n",
    "\n",
    "# convert to numpy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "print(images)\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HaaR Cascade Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect faces in the image and extract the face\n",
    "\n",
    "import cv2\n",
    "haar_file = 'haarcascade_frontalface_default.xml'\n",
    "face_cascade = cv2.CascadeClassifier(haar_file)\n",
    "\n",
    "def face_extractor(img):\n",
    "    faces = face_cascade.detectMultiScale(img, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return None\n",
    "    for (x, y, w, h) in faces:\n",
    "        cropped_face = img[y:y+h, x:x+w]\n",
    "    return cropped_face\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract faces from an image\n",
    "def faces_extractor(img):\n",
    "    faces = face_cascade.detectMultiScale(img, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return None\n",
    "    faces_list = []\n",
    "    for (x, y, w, h) in faces:\n",
    "        cropped_face = img[y:y+h, x:x+w]\n",
    "        faces_list.append(cropped_face)\n",
    "    return faces_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test faces_extractor on \"./dataset/many.jpeg\"\n",
    "# img = cv2.imread('./dataset/many.jpeg')\n",
    "# faces = faces_extractor(img)\n",
    "# for face in faces:\n",
    "#     cv2.imshow('face', face)\n",
    "#     cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# extract faces from the images and display them\n",
    "for image in images:\n",
    "    img = cv2.imread(image)\n",
    "    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    face = face_extractor(img)\n",
    "    if face is not None:\n",
    "        face = cv2.resize(face, (256, 256))\n",
    "        imgplot = plt.imshow(face)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract faces from the images using faces_extractor and display them\n",
    "# for image in images:\n",
    "#     img = cv2.imread(image)\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#     faces = faces_extractor(img)\n",
    "#     for face in faces:\n",
    "#         if face is not None:\n",
    "#             face = cv2.resize(face, (200, 200))\n",
    "#             imgplot = plt.imshow(face)\n",
    "#             plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "for image in images:\n",
    "    img = mpimg.imread(image)\n",
    "\n",
    "    face = face_extractor(img)\n",
    "    if face is not None:\n",
    "        plt.imshow(face)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('Face not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the faces in \"processed\" folder. The folder will be created if it doesn't exist.\n",
    "# the faces are saved with the name of the folder they belong to (label) and a number.\n",
    "processed_path = './processed'\n",
    "if not os.path.exists(processed_path):\n",
    "    os.mkdir(processed_path)\n",
    "#clean the folder\n",
    "for folder in os.listdir(processed_path):\n",
    "    folder_path = os.path.join(processed_path, folder)\n",
    "    for file in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        os.remove(file_path)\n",
    "    os.rmdir(folder_path)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "for i, image in enumerate(images):\n",
    "    img = cv2.imread(image)\n",
    "    face = face_extractor(img)\n",
    "    if face is not None:\n",
    "        face = cv2.resize(face, (256, 256))\n",
    "    \n",
    "        save_path = os.path.join(processed_path, labels[i])\n",
    "        if not os.path.exists(save_path):\n",
    "            os.mkdir(save_path)\n",
    "        file_name_path = os.path.join(save_path, labels[i] + str(i) + '.jpg')\n",
    "        cv2.imwrite(file_name_path, face)\n",
    "        cv2.waitKey(0)\n",
    "    else:\n",
    "        print('Face not found')\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed= './processed/khalil/'\n",
    "processed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #using faces_extractor\n",
    "# processed_path = './processed'\n",
    "# if not os.path.exists(processed_path):\n",
    "#     os.mkdir(processed_path)\n",
    "# #clean the folder\n",
    "# for folder in os.listdir(processed_path):\n",
    "#     folder_path = os.path.join(processed_path, folder)\n",
    "#     for file in os.listdir(folder_path):\n",
    "#         file_path = os.path.join(folder_path, file)\n",
    "#         os.remove(file_path)\n",
    "#     os.rmdir(folder_path)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.image as mpimg\n",
    "\n",
    "# for i, image in enumerate(images):\n",
    "#     img = cv2.imread(image)\n",
    "#     faces = faces_extractor(img)\n",
    "#     for j, face in enumerate(faces):\n",
    "#         if face is not None:\n",
    "#             face = cv2.resize(face, (200, 200))\n",
    "#             face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "#             save_path = os.path.join(processed_path, labels[i])\n",
    "#             if not os.path.exists(save_path):\n",
    "#                 os.mkdir(save_path)\n",
    "#             file_name_path = os.path.join(save_path, labels[i] + str(i) + '_' + str(j) + '.jpg')\n",
    "#             cv2.imwrite(file_name_path, face)\n",
    "#             cv2.waitKey(0)\n",
    "#         else:\n",
    "#             print('Face not found')\n",
    "#             pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning CNN model to recognize face\n",
    "'''This script uses a database of images and creates CNN model on top of it to test\n",
    "   if the given image is recognized correctly or not'''\n",
    "\n",
    "'''####### IMAGE PRE-PROCESSING for TRAINING and TESTING data #######'''\n",
    "\n",
    "# Specifying the folder where images are present\n",
    "TrainingImagePath='./processed'\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# Understand more about ImageDataGenerator at below link\n",
    "# https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "\n",
    "# Defining pre-processing transformations on raw images of training data\n",
    "# These hyper parameters helps to generate slightly twisted versions\n",
    "# of the original image, which leads to a better model, since it learns\n",
    "# on the good and bad mix of images\n",
    "train_datagen = ImageDataGenerator(\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# Defining pre-processing transformations on raw images of testing data\n",
    "# No transformations are done on the testing images\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "# Generating the Training Data\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        TrainingImagePath,\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "\n",
    "# Generating the Testing Data\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        TrainingImagePath,\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "# Printing class labels for each face\n",
    "test_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_set.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the output folder before running the model\n",
    "# folder_path = './output'\n",
    "# for file in os.listdir(folder_path):\n",
    "#     file_path = os.path.join(folder_path, file)\n",
    "#     os.remove(file_path)\n",
    "# os.rmdir(folder_path)\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "dir_Iterator = datagen.flow_from_directory(\n",
    "    TrainingImagePath,\n",
    "    batch_size=32,\n",
    "    save_to_dir=\"output/\",\n",
    "    save_format=\"jpg\",\n",
    "    target_size=(256, 256),\n",
    ")\n",
    "for i in range(10):\n",
    "    img, label = dir_Iterator.next()\n",
    "    print(img.shape, label)\n",
    "    plt.imshow(img[0])\n",
    "    plt.show()\n",
    "\n",
    "training_set=dir_Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training data\n",
    "from keras import preprocessing \n",
    "training_images = image_dataset_from_directory(\n",
    "    TrainingImagePath,\n",
    "    labels='inferred',\n",
    "    class_names=['khalil', 'other'],\n",
    "    color_mode='rgb',\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "#display the images\n",
    "for x, y in training_images.take(1):\n",
    "    plt.imshow(x[0])\n",
    "    plt.show()  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf.keras.preprocessing.image import ImageDataGenerator\n",
    "TrainingImagePath='./processed'\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "dir_Iterator = datagen.flow_from_directory(\n",
    "    TrainingImagePath,\n",
    "    batch_size=32,\n",
    "    save_to_dir=\"output/\",\n",
    "    save_format=\"jpg\",\n",
    "    target_size=(256, 256),\n",
    ")\n",
    "for i in range(10):\n",
    "    img, label = dir_Iterator.next()\n",
    "    print(img.shape, label)\n",
    "    plt.imshow(img[0])\n",
    "    plt.show()\n",
    "\n",
    "training_set=dir_Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a mapping for index and face names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_indices have the numeric tag for each face\n",
    "TrainClasses=training_set.class_indices\n",
    " \n",
    "# Storing the face and the numeric tag for future reference\n",
    "ResultMap={}\n",
    "for faceValue,faceName in zip(TrainClasses.values(),TrainClasses.keys()):\n",
    "    ResultMap[faceValue]=faceName\n",
    "\n",
    "# Print the ResultMap\n",
    "print(ResultMap)\n",
    "\n",
    "# Saving the face map for future reference\n",
    "import pickle\n",
    "with open(\"ResultsMap.pkl\", 'wb') as fileWriteStream:\n",
    "    pickle.dump(ResultMap, fileWriteStream)\n",
    " \n",
    "# The model will give answer as a numeric tag\n",
    "# This mapping will help to get the corresponding face name for it\n",
    "print(\"Mapping of Face and its ID\",ResultMap)\n",
    " \n",
    "# The number of neurons for the output layer is equal to the number of faces\n",
    "OutputNeurons=len(ResultMap)\n",
    "print('\\n The Number of output neurons: ', OutputNeurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the CNN face recognition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    " \n",
    "'''Initializing the Convolutional Neural Network'''\n",
    "classifier= Sequential()\n",
    " \n",
    "''' STEP--1 Convolution\n",
    "# Adding the first layer of CNN\n",
    "# we are using the format (64,64,3) because we are using TensorFlow backend\n",
    "# It means 3 matrix of size (64X64) pixels representing Red, Green and Blue components of pixels\n",
    "'''\n",
    "classifier.add(Convolution2D(32, kernel_size=(5, 5), strides=(1, 1), input_shape=(256,256,3), activation='relu'))\n",
    " \n",
    "'''# STEP--2 MAX Pooling'''\n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
    " \n",
    "'''############## ADDITIONAL LAYER of CONVOLUTION for better accuracy #################'''\n",
    "classifier.add(Convolution2D(64, kernel_size=(5, 5), strides=(1, 1), activation='relu'))\n",
    " \n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
    " \n",
    "'''# STEP--3 FLattening'''\n",
    "classifier.add(Flatten())\n",
    " \n",
    "'''# STEP--4 Fully Connected Neural Network'''\n",
    "classifier.add(Dense(64, activation='relu'))\n",
    " \n",
    "classifier.add(Dense(OutputNeurons, activation='softmax'))\n",
    " \n",
    "'''# Compiling the CNN'''\n",
    "#classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=[\"accuracy\"])\n",
    " \n",
    "###########################################################\n",
    "import time\n",
    "# Measuring the time taken by the model to train\n",
    "StartTime=time.time()\n",
    " \n",
    "# Starting the model training\n",
    "classifier.fit_generator(\n",
    "                    training_set,\n",
    "                    steps_per_epoch=30,\n",
    "                    epochs=10,\n",
    "                    validation_data=test_set,\n",
    "                    validation_steps=10)\n",
    " \n",
    "EndTime=time.time()\n",
    "print(\"###### Total Time Taken: \", round((EndTime-StartTime)/60), 'Minutes ######')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImagePath='/khalil0.jpg'\n",
    "im=cv2.imread(ImagePath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras.utils as image\n",
    " \n",
    "ImagePath='others9.jpg'\n",
    "test_image=image.load_img(ImagePath,target_size=(64, 64))\n",
    "test_image=image.img_to_array(test_image)\n",
    " \n",
    "test_image=np.expand_dims(test_image,axis=0)\n",
    " \n",
    "result=classifier.predict(test_image,verbose=0)\n",
    "#print(training_set.class_indices)\n",
    " \n",
    "print('####'*10)\n",
    "print('Prediction is: ',ResultMap[np.argmax(result)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff39798ba2f49d9c8c85df32f9bbb83f5fb04a27d2e6570577e90ab0f793dad2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
